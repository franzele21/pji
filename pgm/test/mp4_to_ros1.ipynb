{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load Python extension for LZ4 support. LZ4 compression will not be available.\n"
     ]
    }
   ],
   "source": [
    "from cv_bridge import CvBridge\n",
    "import cv2\n",
    "import rosbag\n",
    "import bagpy as bg\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import bagpy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pip3 install --force-reinstall --extra-index-url https://rospypi.github.io/simple/ rospy rosbag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# écriture dans un nouveau fichier bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROS_FILE = \"../../2022-11-17_pietons/Pietons2_1_1.bag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag = rosbag.Bag(ROS_FILE, \"w\")\n",
    "# bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_data = bg.bagreader(\"../../2022-11-17_pietons/Pietons2.bag\")\n",
    "# bridge = CvBridge()\n",
    "# images = bag_data.reader.read_messages(\"/camera/color/image_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     try:\n",
    "#         bag.write(\"/camera/color/image_raw\", next(images).message)\n",
    "#     except:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_data = bg.bagreader(ROS_FILE)\n",
    "# images = bag_data.reader.read_messages(\"/camera/color/image_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# écriture dans un fichier bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_list(box_list: list, frame_rate:int=1, box_difference:int=5):\n",
    "    \"\"\"\n",
    "    Modifies a list of lists (`box_list`) by filling in gaps and ensuring continuity between frames\n",
    "    based on spatial proximity of bounding boxes.\n",
    "\n",
    "    This function iterates through each list of boxes (representing frames) and checks for continuity \n",
    "    of each box between consecutive frames. If a box in a previous frame does not have a close match \n",
    "    in the current frame but has one in subsequent frames (within a specified `frame_rate`), a mean \n",
    "    box is created to bridge the gap.\n",
    "\n",
    "    It will also add a box at before the first occurance of a box, for a more smooth blurring\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    box_list : list of list of list of float\n",
    "        A list where each element is a list representing a frame of bounding boxes, and each bounding \n",
    "        box is a list of floats representing its coordinates.\n",
    "    frame_rate : int, optional\n",
    "        The number of frames to look ahead for matching a box from the previous frame, default is 1.\n",
    "    box_difference : int, optional\n",
    "        The allowed difference between the coordinates of boxes for them to be considered close, \n",
    "        default is 5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of list of list of float\n",
    "        The modified `box_list` with added boxes to ensure continuity between frames.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> boxes = [\n",
    "            [\n",
    "                [10, 10, 20, 20],\n",
    "                [20, 20, 40, 40]\n",
    "            ],\n",
    "            [\n",
    "                [100, 100, 150, 150]\n",
    "            ],\n",
    "            [\n",
    "                [20, 20, 40, 40]\n",
    "            ],\n",
    "            [\n",
    "                [15, 15, 25, 25]\n",
    "            ]\n",
    "        ]\n",
    "    >>> fill_list(boxes, frame_rate=3)\n",
    "    [\n",
    "        [\n",
    "            [10, 10, 20, 20], \n",
    "            [20, 20, 40, 40], \n",
    "            [100, 100, 150, 150]        # was added \n",
    "        ],\n",
    "        [\n",
    "            [100, 100, 150, 150], \n",
    "            [12.5, 12.5, 22.5, 22.5],   # was added\n",
    "            [20.0, 20.0, 40.0, 40.0]    # was added\n",
    "        ],\n",
    "        [\n",
    "            [20, 20, 40, 40], \n",
    "            [13.75, 13.75, 23.75, 23.75]    # was added\n",
    "        ],\n",
    "        [\n",
    "            [15, 15, 25, 25]\n",
    "        ]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    for i in range(1, len(box_list)-1):\n",
    "        # on regarde les boxes de la frame précédente\n",
    "        for last_boxes in box_list[i-1]:\n",
    "            correspondance_now = False\n",
    "            for present_boxe in box_list[i]:\n",
    "                # on trouve une frame ressemblante dans la frame actuelle\n",
    "                if np.isclose(last_boxes, present_boxe, atol=box_difference).all():\n",
    "                    correspondance_now = True\n",
    "            # si on trouve, alors on s'arrête là (pas besoin de créer de boxe)\n",
    "            if correspondance_now:\n",
    "                continue\n",
    "            \n",
    "            # on regarde si les frames d'après ressemble à une box de la frame précédente\n",
    "            correspondance_after = False\n",
    "            for j in range(frame_rate):\n",
    "                if len(box_list) > i+j+1:\n",
    "                    for next_boxe in box_list[i+j+1]:\n",
    "                        # on trouve une frame ressemblante\n",
    "                        if np.isclose(last_boxes, next_boxe, atol=box_difference).all():\n",
    "                            correspondance_after = True\n",
    "                            break\n",
    "                    if correspondance_after:\n",
    "                        break\n",
    "            \n",
    "            # si on trouve une frame ressemblante, alors on créer une approximation entre la\n",
    "            # boxe de la frame précédente et suivante\n",
    "            if correspondance_after:\n",
    "                box_list[i].append(np.mean([last_boxes, next_boxe], axis=0).tolist())\n",
    "\n",
    "        # on rajoute des boxes aux frames précédentes\n",
    "        for present_boxe in box_list[i]:\n",
    "            correspondance = False\n",
    "            for last_boxe in box_list[i-1]:\n",
    "                if np.isclose(last_boxe, present_boxe, atol=box_difference).all():\n",
    "                    correspondance = True\n",
    "            if not correspondance:\n",
    "                box_list[i-1].append(present_boxe) \n",
    "\n",
    "    return box_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_box(frame, box, min_conf=0.3):\n",
    "    x1, y1, x2, y2 = box.xyxy[0]\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    h, w = y2-y1, x2-x1\n",
    "\n",
    "    ROI = frame[y1:y1+h, x1:x1+w]\n",
    "    blur = cv2.GaussianBlur(ROI, (51,51), 0) \n",
    "    frame[y1:y1+h, x1:x1+w] = blur\n",
    "    # frame[y1:y1+h, x1:x1+w] = np.zeros((h, w, 3))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_box1(frame: np.ndarray, \n",
    "             box: list, \n",
    "             black_box: bool=False):\n",
    "    \"\"\"\n",
    "    Apply a blur or black box to a specified region of an image if the confidence level is above a threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame : numpy.ndarray\n",
    "        The image on which the blur or black box is to be applied. It should be a 3D array representing an RGB image.\n",
    "    box : object\n",
    "        An object containing the bounding box coordinates and confidence score. It should have attributes `conf` and `xyxy`:\n",
    "        - `box.conf` : list or numpy.ndarray\n",
    "            The confidence score(s) of the bounding box, with values between 0 and 1.\n",
    "        - `box.xyxy` : list or numpy.ndarray\n",
    "            The coordinates of the bounding box in the format [x1, y1, x2, y2].\n",
    "    black_box : bool, optional\n",
    "        If True, the specified region is filled with a black box instead of being blurred. Default is False.\n",
    "    min_conf : float, optional\n",
    "        The minimum confidence threshold to apply the blur or black box. Default is 0.3. Must in [0, 1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The modified image with the blur or black box applied to the specified region.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The input `frame` must be a 3D NumPy array representing an image with shape (height, width, channels).\n",
    "    - The bounding box coordinates and confidence score must be provided in the `box` object (already implemented in the Yolov8 results).\n",
    "    - If `black_box` is set to True, the region within the bounding box will be replaced with black pixels (faster than blurring).\n",
    "    - The Gaussian blur applied uses a kernel size of (51, 51) with a standard deviation of 0.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    h, w = y2-y1, x2-x1\n",
    "\n",
    "    if black_box:\n",
    "        blur = np.zeros((h, w, 3))\n",
    "    else:\n",
    "        ROI = frame[y1:y1+h, x1:x1+w]\n",
    "        blur = cv2.GaussianBlur(ROI, (51,51), 0) \n",
    "    frame[y1:y1+h, x1:x1+w] = blur\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"models/yolov8n-face.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_conf = 0.2\n",
    "bridge = CvBridge()\n",
    "frame_rate = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# version 1\n",
    "with rosbag.Bag('output.bag', 'w') as outbag:\n",
    "    for topic, msg, t in tqdm(rosbag.Bag(\"../../2022-11-17_pietons/Pietons2.bag\").read_messages()):\n",
    "        if topic == \"/camera/color/image_raw\":\n",
    "            img = bridge.imgmsg_to_cv2(msg, desired_encoding=\"rgb8\").copy()\n",
    "            boxes = next(model(img, stream=True, verbose=False)).boxes\n",
    "            for box in boxes:\n",
    "                img = blur_box(img, box)\n",
    "            image_message = bridge.cv2_to_imgmsg(img, encoding=\"passthrough\")\n",
    "            outbag.write(topic, image_message, msg.header.stamp)\n",
    "        else:   \n",
    "            outbag.write(topic, msg, msg.header.stamp if msg._has_header else t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# version 2\n",
    "with rosbag.Bag('output.bag', 'w') as outbag:\n",
    "    last_imgs = []\n",
    "    begin = True\n",
    "    save_imgs = False\n",
    "    for topic, msg, t in tqdm(rosbag.Bag(\"../../2022-11-17_pietons/Pietons2.bag\").read_messages()):\n",
    "        if topic == \"/camera/color/image_raw\":\n",
    "            img = bridge.imgmsg_to_cv2(msg, desired_encoding=\"rgb8\").copy()\n",
    "            last_imgs.append(img)\n",
    "\n",
    "            if begin and len(last_imgs) == math.ceil(frame_rate/2):\n",
    "                begin = False\n",
    "                save_imgs = True\n",
    "                idx_img = 0\n",
    "            elif len(last_imgs) == frame_rate:\n",
    "                save_imgs = True\n",
    "                idx_img = math.ceil(frame_rate/2)-1\n",
    "\n",
    "            if save_imgs:\n",
    "                save_imgs = False\n",
    "                boxes = next(model(last_imgs[idx_img], stream=True, verbose=False)).boxes\n",
    "                to_blur = len(boxes) > 0\n",
    "                for img in last_imgs:\n",
    "                    if to_blur:\n",
    "                        boxes = next(model(img, stream=True, verbose=False)).boxes\n",
    "                        \n",
    "                        for box in boxes:\n",
    "                            if math.ceil((box.conf[0]*100))/100 > min_conf:\n",
    "                                img = blur_box(img, box)\n",
    "                    image_message = bridge.cv2_to_imgmsg(img, encoding=\"passthrough\")\n",
    "                    outbag.write(topic, image_message, msg.header.stamp)\n",
    "                last_imgs = []\n",
    "\n",
    "        else:   \n",
    "            outbag.write(topic, msg, msg.header.stamp if msg._has_header else t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174963it [00:32, 5333.71it/s]\n",
      "174963it [00:03, 54941.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%script false --no-raise-error\n",
    "min_conf = None\n",
    "\n",
    "# version 3\n",
    "with rosbag.Bag('output.bag', 'w') as outbag:\n",
    "    last_imgs = []\n",
    "    boxes_list = []\n",
    "    begin = True\n",
    "    save_imgs = False\n",
    "    for topic, msg, t in tqdm(rosbag.Bag(\"../../2022-11-17_pietons/Pietons2.bag\").read_messages()):\n",
    "        if topic == \"/camera/color/image_raw\":\n",
    "            img = bridge.imgmsg_to_cv2(msg, desired_encoding=\"bgr8\").copy()\n",
    "            last_imgs.append(img)\n",
    "\n",
    "            if begin and len(last_imgs) == math.ceil(frame_rate/2):\n",
    "                begin = False\n",
    "                save_imgs = True\n",
    "                idx_img = 0\n",
    "            elif len(last_imgs) == frame_rate:\n",
    "                save_imgs = True\n",
    "                idx_img = math.ceil(frame_rate/2)-1\n",
    "\n",
    "            if save_imgs:\n",
    "                save_imgs = False\n",
    "                boxes = next(model(last_imgs[idx_img], stream=True, verbose=False)).boxes\n",
    "                to_blur = len(boxes) > 0\n",
    "                for img in last_imgs:\n",
    "                    boxes_list.append([])\n",
    "                    if to_blur:\n",
    "                        boxes = next(model(img, stream=True, verbose=False)).boxes\n",
    "                        if len(boxes.conf) > 0:\n",
    "                            if isinstance(min_conf, type(None)):\n",
    "                                min_conf = boxes.conf.mean() * 0.7\n",
    "                            else:\n",
    "                                min_conf = (min_conf + boxes.conf.mean())/2 * 0.7\n",
    "                        for box in boxes:\n",
    "                            if box.conf[0] > min_conf:\n",
    "                                boxes_list[-1].append(box.xyxy[0].tolist())\n",
    "                last_imgs = []\n",
    "        else:   \n",
    "            outbag.write(topic, msg, msg.header.stamp if msg._has_header else t)\n",
    "    # on vérifie les dernières frames\n",
    "    boxes = next(model(last_imgs[-1], stream=True, verbose=False)).boxes\n",
    "    to_blur = len(boxes) > 0\n",
    "    for img in last_imgs:\n",
    "        boxes_list.append([])\n",
    "        if to_blur:\n",
    "            boxes = next(model(img, stream=True, verbose=False)).boxes\n",
    "            \n",
    "            for box in boxes:\n",
    "                boxes_list[-1].append(box.xyxy[0].tolist())\n",
    "    \n",
    "    boxes_list = fill_list(boxes_list, frame_rate*2, math.ceil(math.sqrt(max(msg.width, msg.height)))*2)\n",
    "    idx_boxes = 0\n",
    "    for topic, msg, t in tqdm(rosbag.Bag(\"../../2022-11-17_pietons/Pietons2.bag\").read_messages()):\n",
    "        if topic == \"/camera/color/image_raw\":\n",
    "            img = bridge.imgmsg_to_cv2(msg).copy()\n",
    "            for box in boxes_list[idx_boxes]:\n",
    "                img = blur_box1(img, box)\n",
    "            image_message = bridge.cv2_to_imgmsg(img, encoding=\"rgb8\")\n",
    "            outbag.write(topic, image_message, msg.header.stamp)\n",
    "            idx_boxes += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  Data folder output already exists. Not creating.\n"
     ]
    }
   ],
   "source": [
    "bag_data = bg.bagreader(\"output.bag\")\n",
    "images = bag_data.reader.read_messages(\"/camera/color/image_raw\")\n",
    "fps = bag_data.topic_table[bag_data.topic_table[\"Topics\"] == \"/camera/color/image_raw\"][\"Frequency\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]  Data folder output already exists. Not creating.\n",
      "(640, 480)\n"
     ]
    }
   ],
   "source": [
    "topic = \"/camera/color/image_raw\"\n",
    "# bag_data = bg.bagreader(\"../../2022-11-17_pietons/Pietons2.bag\")\n",
    "bag_data = bg.bagreader(\"output.bag\")\n",
    "\n",
    "bridge = CvBridge()\n",
    "images = bag_data.reader.read_messages(topic)\n",
    "\n",
    "fps = bag_data.topic_table[bag_data.topic_table[\"Topics\"] == \"/camera/color/image_raw\"][\"Frequency\"].item()\n",
    "\n",
    "frame = next(images).message\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video = cv2.VideoWriter(filename=\"supp.mp4\", \n",
    "                    fourcc=fourcc, \n",
    "                    fps=fps, \n",
    "                    frameSize=(frame.width, frame.height))\n",
    "print((frame.width, frame.height))\n",
    "while True:\n",
    "    try:\n",
    "        video.write(bridge.imgmsg_to_cv2(frame, desired_encoding=\"bgr8\"))\n",
    "        frame = next(images).message\n",
    "    except StopIteration:\n",
    "        break\n",
    "\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/franzele/Desktop/univ_lille/m1s2/pji/pgm/test'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
